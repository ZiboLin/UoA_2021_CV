{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: image classification (40%)\n",
    "\n",
    "In this question we will train a neural network to classify images of clothing. We will use Keras, a deep learning toolkit that is built on TensorFlow (developed at Google). You can install Keras and Tensorflow libraries on your local machine. However, you may find it more convenient to use [Google's Colab environment](http://colab.research.google.com/http://colab.research.google.com/) for this question as all required libraries are pre-installed, and it may run faster than your computer. You can upload and download local notebooks to Colab but you should always save a recent version locally, for safety.\n",
    "\n",
    "We will use the Fashion-MNIST dataset which is available in Keras. A tutorial that explains the dataset and the overall workflow of training an image classifier in Keras is available here:\n",
    "\n",
    "https://www.tensorflow.org/tutorials/keras/classification\n",
    "\n",
    "I highly recommend that you go through this first to get a good background understanding for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "id": "a_hd2FbwRZa5",
    "outputId": "78093490-70f7-4694-c511-425e7adeee83"
   },
   "outputs": [],
   "source": [
    "# This makes figures that show how the training and testing accuracy and loss\n",
    "# evolved against the number of epochs for the current training run\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_history(h):\n",
    "\n",
    "    plt.plot(h.history['accuracy'])\n",
    "    plt.plot(h.history['val_accuracy'])\n",
    "    plt.legend((\"train accuracy\",\"test accuracy\"))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(h.history['loss'])\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    plt.legend((\"train loss\",\"test loss\"))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and pre-process the images so all pixels are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "biOI1vLBMnQ_"
   },
   "source": [
    "## Training the network\n",
    "\n",
    "In the cell below a lot is happening. \n",
    "\n",
    "- learning_rate: This determines how quickly the network updates it weight in response to the incoming gradients. Change too slowly and the network may never reach the lowest loss value, change too fast and you run into the danger of oscillating. Typical values range between very small (1e-5) to 0.1\n",
    "\n",
    "- max_epochs: One epoch is a pass over the whole training set. Setting this number tell the training algorithm to do this many passes over the whole data. \n",
    "\n",
    "### Network definition\n",
    "\n",
    "Line 4-7 define the architecture of the network. \n",
    "\n",
    "- Line 4: We tell keras that the model will be of the _Sequential_ type, that is data is going to flow from the input to the output and we do not have any forks / loops.\n",
    "\n",
    "- Line 6: In keras, Dense means a fully connected layer. To our model we add a Dense Layer, with 64 neurons. Assuming our input is $x$, the output after the fully connected layer will be of the form $y_1 = W_1x$\n",
    "Another important thing is the *activation* parameter, which we have set to sigmoid. This is the non-linearlity which will be applied to the **output** of this layer that is $y_{\\sigma_{1}} = \\sigma_{1}(W_{1}x)$.\n",
    "\n",
    "- Line 7: We additionally have another layer which maps the output $y_{\\sigma_{1}}$ to a single output, with another sigmoid as the activation function. The output of this sigmoid is used to classify if the class is 0 or 1. (-1 or 1 in case of Keras, but that conversion happens automatically and we do not need to worry about it.)\n",
    "\n",
    "- Line 9: Just an architecture is not enough for learning. We need to specify a **loss function** as well as an optimizer. For this assignment, we start with Adam (an efficient version of Stochastic Gradient Descent) as the optimizier. However, we can choose different losses and see their effect on how we learn. All of this is brought together using **compile** in Keras.\n",
    "\n",
    "- Line 13 is where the training happens. This done by calling the fit() method of the model with the training data (train_images and train_labels). We also pass the testing data to see how well we are doing along the way. This is just for evaluation and the network never uses this data to train. Once run, this will print a line every epoch to report loss and accuracy for both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minlin/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5592 - accuracy: 0.8098 - val_loss: 0.4726 - val_accuracy: 0.8335\n",
      "Epoch 2/25\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4191 - accuracy: 0.8516 - val_loss: 0.4340 - val_accuracy: 0.8463\n",
      "Epoch 3/25\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3771 - accuracy: 0.8655 - val_loss: 0.4432 - val_accuracy: 0.8414\n",
      "Epoch 4/25\n",
      " 926/1875 [=============>................] - ETA: 1s - loss: 0.3506 - accuracy: 0.8742"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "max_epochs = 25\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(tf.keras.layers.Dense(64, activation='ReLU'))\n",
    "model.add(tf.keras.layers.Dense(64, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "h = model.fit(train_images, \n",
    "              train_labels, \n",
    "              epochs=max_epochs, \n",
    "              validation_data = (test_images, test_labels))\n",
    "\n",
    "eval_results = model.evaluate(test_images, test_labels, verbose=0) \n",
    "print(\"\\nLoss, accuracy on test data: \")\n",
    "print(\"%0.4f %0.2f%%\" % (eval_results[0], eval_results[1]*100))\n",
    "\n",
    "plot_train_history(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with the network\n",
    "\n",
    "The following questions require you to train and evaluate a model with several different settings. You should modify the example code above so that it is convenient for you to run your tests repeatedly. For each question, start with the base case when performing comparisons.\n",
    "\n",
    "1. The current architecture has a single hidden layer with 64 neurons in it. We can add more neuron in this hidden layer (try doubling it for example), this will make the layer “wider”. Alternatively, we can add an additional layer. Compare and contrast the performance of these two approaches. How many parameters does the network contain for each setting you tried?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just some notes on the testing above \n",
    "# original values: dense layer with 64 and 10 neurons, with sigmoid activation function \n",
    "#     result: 0.3426 87.63%\n",
    "# double neuron in the hidden layer:\n",
    "#     result: 0.3286 88.15% we can see from the sample that, the accuracy osciallte a lot \n",
    "# adding one more layer:\n",
    "#     result: 0.3419 87.82%\n",
    "# using 2000 neurons:\n",
    "#     result: 0.3156 89.15%\n",
    "# Adding five more layers: \n",
    "#     result: 0.4539 84.42%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here \n",
    "# Doubling the neuron in the hidden layer did not do much in terms of improving our accuracy\n",
    "#   and loss, but if we increase the neurons drastically(2000 neurons), we can see there is\n",
    "#   huge improvement both on loss and accuracy. \n",
    "\n",
    "# Adding more hidden layer actually reduce both the accuracy and loss.\n",
    "\n",
    "# Adding more parameters is usually good for our model, but if we have too much, \n",
    "#   such as having a wide model, it puts more pressure on our learning.\n",
    "\n",
    "# If the model is too deep, it will increase our rate of learning and calculation speed,\n",
    "#   it is also hard to train, because it often oscillate between boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Change the **activation function** on the intermediate layers and measure the effect on accuracy and convergence rate. Discuss your results. In the lectures we saw that ReLU usually learns faster than sigmoids, does this hold true? How many epochs does the ReLU based training converge in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "#case 1: Sigmoid activation function with 64 neurons \n",
    "#     result: 0.3426 87.63%\n",
    "\n",
    "#case 2: ReLU activation function with 64 neurons\n",
    "#     result: 0.3509 87.47%\n",
    "\n",
    "#case 3: Leaky ReLU activation function with 64 neurons\n",
    "#     result: 0.3837 86.10%\n",
    "\n",
    "#case 4: tanh activation function with 64 neurons\n",
    "#     result: 0.3330 88.26%\n",
    "\n",
    "#Since using an epochs of 10 produce very similar result, we move onto using 50 epochs\n",
    "\n",
    "#case 1: Sigmoid activation function with 64 neurons \n",
    "#     result: 0.3847 88.42%\n",
    "\n",
    "#case 2: ReLU activation function with 64 neurons\n",
    "#     result: 0.4341 88.20%\n",
    "\n",
    "#case 3: ReLU activation function with 3 layers of 64 neurons\n",
    "#     result: 0.3912 89.02%\n",
    "\n",
    "# From observing the patterns, ReLU and sigmoid both converged when epochs is around 20\n",
    "\n",
    "# Is it not neccessary the case that ReLU will learn faster than Sigmoid, it will need to depend on the network \n",
    "#   that it is trained with. In theory, ReLU does compute better than Sigmoid, since ReLU only need to pick \n",
    "#   max(0,x)and not peform expensive exponential operation as in sigmoid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Vary the **learning rate** and show its effect on the accuracy and convergence speed of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "#case 1: ReLU activation function with 64 neurons, lr = 0.001, epochs = 25\n",
    "#     result: 0.3303 88.34%\n",
    "\n",
    "#case 2: ReLU activation function with 64 neurons, lr = 0.005, epochs = 25\n",
    "#     result: 0.4000 86.57%\n",
    "\n",
    "#case 3: ReLU activation function with 64 neurons, lr = 0.01, epochs = 25\n",
    "#     result: 0.4361 84.93%\n",
    "\n",
    "#case 4: ReLU activation function with 64 neurons, lr = 0.1, epochs = 25 \n",
    "#     result: 1.0204 66.76%\n",
    "\n",
    "#case 5: ReLU activation function with 64 neurons, lr = 0.0001, epochs = 25\n",
    "#     result: 0.3810 86.82%\n",
    "\n",
    "#case 6: ReLU activation function with 64 neurons, lr = 0.00001, epochs = 25 \n",
    "#     result: 0.5609 81.47%\n",
    "\n",
    "#case 7: ReLU activation function with 64 neurons, lr = 0.0005, epochs = 25 \n",
    "#     result: 0.3315 88.09%\n",
    "\n",
    "#case: we set learning rate to be around 0.001 but its accuracy is not better than when it is set to 0.001\n",
    "\n",
    "\n",
    "# We can see that when the learning rate start to increase from 0.001 , the accuracy actually decrease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many epochs should the training run for? Justify your answer by making observations about convergence during your experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here ，\n",
    "# we need to increase the number of epoch before it has a drastically increase of error(overfitting)\n",
    "# Therefore, we need to observe on where in specific does the accuracy starts to increase in a slower rate and \n",
    "#   lost start to increase.\n",
    "# From the test case above, we see that when epochs reach between 19-25, it has the lowest loss, and slower \n",
    "#   increase in accuracy, therefore, we set the maximum epochs to 25. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, report your best combination of architecture, loss, activation function and learning rate, and evaluate your model with these settings. Discuss your result and the trade-off between classification accuracy and time/resources required to train your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# \n",
    "\n",
    "# From the test cases, \n",
    "#   Using deeper or wider layer will not make a huge improvement on the accuracy of the trained data\n",
    "#     (It does make improvement, but consider the cost of using wider and deeper network, even wider network)\n",
    "#     we still use the original structure, 64 neurons for the single hidden layer, since it is \"good enough\"\n",
    "\n",
    "#   We can see that sigmoid and ReLU gives very similar outcome, therefore in the later test\n",
    "#     cases, we mainly used ReLU since it gives the least computational cost. \n",
    "#\n",
    "#   For the learning rate, We can see that when the learning rate start to increase from 0.001 , \n",
    "#     the accuracy actually decrease. Therefore, the learning rate is set to be 0.001\n",
    "\n",
    "\n",
    "# If we implement too many epochs, then its will take longer to compute, but if we can let it stop when converged \n",
    "#   it will be a better option. For this example, since the training set always converged around 25, therefore we\n",
    "#   set it to be 25. \n",
    "\n",
    "# Adding more parameters(doubling neurons) is usually good for our model, but if we have too much,  such as  \n",
    "#   having a wide model,it puts more pressure on our learning.\n",
    "\n",
    "# If the model is too deep(too many layers), it will increase our rate of learning and calculation speed,\n",
    "#   it is also hard to train, because it often oscillate between boundary.\n",
    "\n",
    "#Therefore, it is not only to build a model that has the best accuracy, but also runtime is very important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgraduate students only (10%, other questions scaled to 90%)\n",
    "\n",
    "Based on your previous experiments, implement another modification of your choice to improve the performance/accuracy tradeoff. Explain what you have done, why you chose to do it, and what effect it had on performance/accuracy. (Hint: even if it does not ultimately improve performance, the explanation and measurement is what is important for this question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
